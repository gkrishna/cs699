% Chapter Template

\chapter{Background and Related Work} % Main chapter title

\label{relatedWork} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 2. \emph{Background and Related Work}} 

With the advancement of the machine learning algorithms, there have also been increase in the attempts of trying various innovative attempts of classifying images. There have been attempts to develop low level image features to have faster classification. Also people tried to mimic the human visual perception process in image classification. Most of the image classification solutions counter with an unanimous problem the deficiency of sufficient data. It is really difficult to have image data for all classes. Even if you get data, you have to deal with problems of noisy tags, ambiguous content etc. On the other side, we have lot of image data getting created on world wide web. This leads to a simple question that can not we use this data get some semantic structure, which would help us in image classification. The answer is yes, but the extent to which we can structure the world wide web and extract out the useful information still has lot of research potential.\\ 
\hspace*{1cm} The important concept, we want to coin here is heterogeneous learning. Kesorn, Kraisak \cite{kesorn}, as shown that although, text and visual are distinct types of representation and modality. There are some strong implicit connections between images and any accompanying text information. Semantic analysis of image captions can be used by image retrieval systems to retrieve selected images more precisely. This is a form of heterogeneous learning. They had first extracted meta data automatically from text captions and restructured with respect to a semantic model. Second, they used Latent Semantic Indexing to create a domain-specific ontology-based knowledge model. This enables the framework to tolerate ambiguities and variations (incompleteness) of meta data.\\
%
\hspace*{1cm} In \cite{Gupta}, Kim, Kristen Grauman and Raymond Mooney solved the problem of learning robust models out of scenes and actions from partially labeled collections. They proposed to leverage the text accompanying visual data to cope with these constraints. They concluded that exploiting the multi-modal representation and unlabeled data provides more accurate image and video classifier compared to base-line algorithms. They have also asserted that this extra data and multi-modal representation can be formulated as a basis of a solution to the problem of managing the world's ever growing multi-media data of digital images and videos.\\
%Image + Social metadata for image classification
\hspace*{1cm} Julian McAuley and Jure Leskovec \cite{McAuley} has proposed that how we can use the inter-dependencies of image sharing common properties in multi-modal classification settings for image labeling in social networks. They used the same Large Scale Image Retrieval benchmarks (MIR, PASCAL, CLEF and NUS), which we are using in the thesis. They modeled their task as binary labeling problem on a network and used some machine learning technique to create a binary labeling model. They have analyzed about the relative importance of social features (eg. shared membership in a gallery, relational feature on the based on shared location, shared group etc.) in image labeling.\\
\hspace*{1cm} Stvilia, Besiki and J\"{o}rgensen, Corinne and Wu, Shuheng \cite{imageIndexing} came up with a framework for evaluating the added value of socially created meta-data to image indexing process. They were trying to define content models understandable and search-able for humans. Image indexing becomes a complex socio-cognitive task because it includes processing available data, then classifying it, abstracting and mapping that data into semantic concepts and entities often expressed through, which should also be justified on the basis of human linguistics. They have shown that how the meta-data or tags obtained by social network helps in understanding the higher level concepts and semantic relationships hidden in images, which would otherwise be not achievable.\\
\hspace*{1cm} It has been asserted that collaborative tagging, social classification, social tagging, which is also said "Folksonomy" \cite{wikiFolksonomy}, holds the key to develop a semantic web, in which every web page contains machine readable meta-data that describes its content. Such meta-data can be harnessed to describe the content and classify the content.  In \cite{webResources}, it has shown the impact of the meta-data for retrieving information from web-site.\\
%%
\hspace*{1cm}Oded Nov and Mor Naaman \cite{nov} and Kern, R. and Granitzer, M. and Pammer, V. \cite{kern}, have experimented on Folksonomy.
They used some collaboratively created sets of meta-data, to organize multimedia information available on the Web. They addressed question how to extend a classical folksonomy with additional metadata. They also shown that it can be applied for tag recommendation.\\
%%
\hspace*{1cm} Ed H. Chi and Todd Mytkowicz \cite{chi2008understanding} has systematically analyzed the efficiency of social tagging systems using information theory. They try to find the answer about the efficacy of naturally evolved user generated vocubalary in identifying the objects (images, videos etc.).They have shown that information theory improvises an interesting way to understand the descriptive power of tags. Their results show that even information theory gives evidence that social tags can be used to identify the objects.\\
\hspace*{1cm} Liu, Dengsheng Zhang, Lua, and Wei-Ying \cite{liu} has demonstrated that In order to improve the retrieval accuracy of content-based image retrieval systems, we can fuse the data from HTML text and visual content of images for World-Wide-Web retrieval.\\
\hspace*{1cm} Yuan Lin and Yuqiang Chen et al \cite{heterogenous} has shown that how labeled text form web  help image classification. In this paper, they have investigated the interplay between multimedia data mining and text data mining. They addressed the problem of image classification with limited amount of labeled images and large amount of auxiliary labeled text data. They have considered the bag of Words model and Navie Bayesian Classification models. They have estimated the image feature mapping on the text-image co-occurrence data, acting like a bridge, which connects the text knowledge to image.\\
\hspace*{1cm} In \cite{textTransformed}, Nuo Zhang and Toshinori Watanabe has proposed a text-transformed image classification. They have proposed to first transform the images into texts. Then these text-transformed images are divided into segments and replaced by characters. Then, they use a novel method of using the similarity between compressibility vectors of texts is the basis of classification.\\
\hspace*{1cm} In 2010, Slaney and Mahajan \cite{dhruv} combined the information from the social graph with some semi-supervised techniques from all the unclassified images to create an enhanced image-classification model for multimedia data. They have shown that fusing image, text and social-graph features gives a large improvement over content features alone in an experiment, where they tried to classify the imahes of adults among all the images.\\
\hspace*{1cm}  Leandro et al. \cite{silva} has shown a neural approach based on swarms to combine visual feature and text data for classifying the images.They has shown it as a two step process, one in which they query from text related to the image and in next they query for image characteristics to fulfill the objective of representing multimedia data and to extract knowledge from this kind of data.\\
\hspace*{1cm} In IEEE Conference on Computer Vision \& Pattern Recognition 2010, Matthieu Guillaumin, Jakob Verbeek and Cordelia Schmid has considered a scenario where keywords are associated with the training images, eg as found on photo sharing websites. They demonstrated a semi-supervised multi-modal learning for image classification. They have shown that how the other source of information can aid the learning process when we have limited number of labeled images.They used PASCAL 2007 dataset and tried learning classed from Flickr Tags.\\
\hspace*{1cm} In \cite{raina} Honglak and Packer et al. has introduced a concept of self-taught learning. In this, they have presented a new machine learning framework. In this framework, they used unlabeled data used in supervised form to do classification tasks. They used a large number of unlabeled images, audio samples or text documents downloaded from the internet to improve performance on a given image classification task.\\
\hspace*{1cm} Adam Rae and Zwol van \cite{rae} addressed the task of recommending the additional tags to partially annotated images. For this they proposed to used the various collective contexts. They used a large-set of real world data from Flickr to show that collective social knowledge has capability of significantly improve the tag recommendation.\\
\hspace*{1cm} Zwol van \cite{vanZwol} handled the problem of predicting of users' favorite photos in Flickr. They used a multi-modal machine learning approach, which fuses social, visual and textual signals into a single prediction system. They proposed that the  visual, textual and social modalities effectively infer the needs of most users. They used gradient-boosted decision trees (GBDT) for the classification of a user's favorite photos. For the evaluation of the performance of their classifier, they classified the data with respect to the individual modalities and various combinations. By using heterogeneous modalities, the GBDT becomes a highly effective classifier. The addition of textual features helps to  significantly boost the recall, with a bit decrease in precision.\\
\hspace*{1cm} Jiayu Tang and Paul H. Lewis \cite{tang} proposed an visual image based feature space; mapping  image regions and textual labels into one single space. They proposed that such representation makes the object recognition very straightforward. In this space, similar image segments linked with the same objects are clustered together, and also lie in the neighborhood of the labels linked to these objects.\\
\hspace*{1cm} In CVPR Paper\cite{Boutell2005935} of 2004, Matthew Boutell and Jiebo Luo shown that how we can leverage the camera meta data to provide evidences independent of the captures scene content; to improve the classification performance. They used Bayesian network to fuse content based and metadata features. Their results demonstrates that this integration of camera meta data can only increases the efficacy of classification.\\
\hspace*{1cm} In 1994, Bartell and Cottrell \cite{Bartell:1994:ACM:188490.188554} has shown that we can have combine multiple retrieval systems to get a superior retrieval model. They demonstrated that if we use multi-modal query to retrieve a single informational entity, we get more effective superior retrieval efficacy.\\
\hspace*{1cm} In \cite{socialLDA}, Teredesai and Bindra has shown the variety of methods for scalable topic modeling in social networks.  They have talked about using  Latent Dirichlet allocation (LDA), Latent Semantic Indexing (LSI) etc. unsupervised topic modeling techniques to  harness social linkages to decipher the user interests for target recommendation.