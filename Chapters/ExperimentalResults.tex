% Chapter 1

\chapter{Experimental Results} % Main chapter title

\lhead{Chapter 5. \emph{Experimental Result}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
In this chapter, we first give an overview of the visual and social feature vectors constructed and after that we discuss the classifiers tested. In remaining part of chapter, we describe the results on four benchmark data-sets.

\section{Feature Vectors}
We extracted the five image features SIFT, GIST, HOG-LBP, CIELAB color space vector and
GLCM as described in the previous chapters. Apart from these visual features, we construct two feature vectors on the basis of social meta data. These feature vectors are constructed after doing analysis of social data and obtaining a binary feature vector, indicating the presence or absence of a social/textual element for the image.
     This high dimensional binary vector leads to two different feature vectors. One which is made of dimensionality reduction by Latent Semantic Indexing and second by which is the resultant of Random Projections.


%----------------------------------------------------------------------------------------

\section{Classifiers Used}

After experimenting with various classifiers like Random Forest, MLP (Multi Layer Perceptron), libSVM (with various kernels linear, RBF, $\chi^2$, histogram intersection), we found out that in case of image features libSVM with $\chi^2$ kernel works best and in case of social features libSVM with the linear kernel gave us the best results. We,therefore, use libSVM with $\chi^2$ kernel as classifier for visual feature vectors and libSVM with Linear Kernel as classifier for social feature vectors.


\section{Classification Results}

In the following part of this chapter, we have shown classification results on various labels of four data sets as mentioned in \ref{Chapter3} .\\

The results are divided in four subsections according to four data-sets. We have shown classification results from all the features extracted and then provided the result of ensemble of all those feature.\\ 

We have given a qualitative and quantitative conclusion/observation of our results. We have also shown a comparison with the results directly published material on each of these four benchmark, or associated competition.\\ 
The goal of all these comparison is to assess the improvement that can be obtained by using social meta-data of images. We report the mean average precision (MAP) for the sake of comparison with published materials and competition results. We also give the accuracy for this binary prediction/classification of labels.\\
All these results are for tenfold cross validation. For ensemble method, we have divided the data in three parts: training, testing and validation set. We randomly chose the 10\% feature instances for validation set and learn weights for the linear combination of the classifiers, which provide best results. After learning those weights we use these optimized combination of these classifiers to do classification testing on test set.\\

\subsection{MIR Flickr collection }
%----------------------------------------------------------------------------------------
MIR has images has high quality photographic images. It has a rich meta data attached with it. This provides a wide variety of image retrieval bench-marking scenarios.

  In \cite{MIRresults}, a combination of social data and low-level content-based descriptors to improve the accuracy of visual concept classifiers. We use the results of this paper as a comparison metric for our results.
 In \cite{MIRresults}, they have used the following four sets of image features:
\begin{itemize}
\item HMMD Color Histogram descriptor.
\item Spatial Color Mode descriptor.
\item MPEG-7 Edge Histogram
\item MPEG-7 Homogeneous Texture descriptor
\end{itemize}


Apart for these low-level content based descriptors they also use flicker tags of visual concepts.  A set consisting of 293 binary features is developed using these tags. The tags corresponds with at least 50 images in the MIR Flickr collection.\\
They have further compared the classification accuracy between classifiers based on low-level features only and classifiers that additionally use the Flicker tags (Set 5 above) as features.\\
They have used two classifiers one is Linear Discriminant Analysis and other is support vector machines. \\

We have shown the map comparison of our computation and result of \cite{MIRresults}in table\ref{table:MIRPrecision}.\\
The accuracy obtained with our computation is shown in table \ref{table:MIRAccuracy}.\\


\subsubsection*{Observations}
\begin{itemize}
\item The classifications based on visual only features give the average precision of 76.15\%, which outperforms the low level image descriptor based classification with 40.43\% in case of LDA and 44.38\% in case of SVM. The result is also better than classification based on the combination of low level image descriptors and flicker tags. Here we see a precision increment of 28.28\% as compared to SVM results in paper and 26.40\% as compared to LDA results in paper.
\item When we do classification on the basis of social features computed using LSI, we get an average precision of 87.59\%. This is 39.71\% more compared to SVM classification of combined features of flicker tags and low level image descriptors and  37.83\% more compared to LDA classification of this combined feature set.
\item LSI based social feature classification outperforms our visual only classification with 11.43\% precision increment.  It shows a precision gain of 51.86\% and 55.81\% on the low level image descriptor based SVM classification and LDA classification respectively.

\item Ensemble of the social features and visual features provides average precision of 90.58\% which is 40.83\% more compared published result and 2.99\% more compared to LSI method.

\item This 42.70\% precision gain compared to published result is coherent with the results shown in [Jure paper]. They obtained a precision gain of 42\% using the social features.

\item Our LSI based method works better for all labels except \textit{Clouds}. The images in this label shows better result with HOG\_ LBP features. This is due to low volume of social information attached with the data and the high visual concepts entailed in these images, which is more emphasized by HOG-LBP features.

\item The results of social features and image features are quite close for \textit{night, tree, sea} and \textit{river}. This is the result of a great degree of visual information present with these images contrast to social information in comments (or other social entities), as they are natural outdoor photographs.

\item The classifications based on visual only features give the average accuracy of 76.03\%.

\item The classifications based on social only features with LSI pre-computation give average accuracy of 86.25 outperforming the visual features only method with 10.22\%.

\item The fusion of all the features provide an average accuracy of 88.80\% outperforming the social feature only accuracy with 2.55\%.


\item GLCM plays an important role in the case of \textit{Bird , male , baby}  labels.

\item GIST plays a pivotal role in case of labels \textit{female, male} and \textit{tree}.


\end{itemize}




\newpage

\begin{sidewaystable}
\caption{ MIR: Precision Comparison} % title of Table % used for centering table
\vspace*{0.2 cm}
\begin{tabular}{|c|p{1.7cm}| p{1.7cm}| p{1.7cm}| p{1.7cm}|p{1.2cm}| p{1.2cm}|p{1cm}| p{1cm}|p{1cm}| p{1cm}|p{1.4cm}| p{1.2cm}||} \\  [0.5ex] \hline
Labels  & Ensemble Method  &  \multicolumn{4}{c}{MIR Published Results} & \multicolumn{7}{c}{Individual Feature Based Classificatiom}\\  [0.5ex] \hline
  &  &\multicolumn{2}{c}{SVM}	&	\multicolumn{2}{c}{LDA}  & \multicolumn{2}{c}{Social Features}  & \multicolumn{5}{c}{Visual Features}\\  [0.5ex] \hline
  & & Flicker tags $+$ Image Descriptors & Image Desciptors Only	& Flicker tags $+$ Image Descriptors& Image Desciptors Only	& LSI & RP & HOG-LBP & SIFT & GIST & COLOR & GLCM \\  [1ex] \hline
flower & 92.96 & 48.00 & 46.90 & 56.00 & 30.10 & 91.37 & 74.34 & 78.94 & 41.60 & 73.19 & 70.37 & 64.40 \\  [1ex]
car & 96.91 & 33.90 & 17.90 & 29.70 & 14.20 & 92.21 & 72.68 & 83.27 & 56.54 & 71.70 & 64.35 & 76.65 \\  [1ex]
bird & 95.77 & 44.30 & 12.80 & 42.60 & 9.70 & 93.37 & 79.26 & 70.11 & 47.96 & 61.77 & 60.63 & 71.17 \\  [1ex]
dog & 98.15 & 60.70 & 15.50 & 62.10 & 10.80 & 95.94 & 73.38 & 73.68 & 47.39 & 68.80 & 65.31 & 67.67 \\  [1ex]
night & 90.53 & 58.80 & 55.40 & 61.50 & 51.50 & 87.81 & 71.85 & 82.40 & 58.00 & 70.40 & 79.16 & 81.37 \\  [1ex]
tree & 84.05 & 55.90 & 51.40 & 51.50 & 43.40 & 81.43 & 69.18 & 76.92 & 54.59 & 74.97 & 61.64 & 76.11 \\  [1ex]
clouds & 92.00 & 69.50 & 65.10 & 65.10 & 57.70 & 82.75 & 74.12 & 90.91 & 50.88 & 79.78 & 67.31 & 76.55 \\  [1ex]
portrait & 84.08 & 48.00 & 49.30 & 54.30 & 43.20 & 83.54 & 71.20 & 69.31 & 50.61 & 65.30 & 59.99 & 65.85 \\  [1ex]
female & 83.55 & 46.40 & 46.10 & 49.40 & 40.40 & 81.52 & 69.90 & 63.72 & 52.56 & 62.27 & 54.11 & 58.77 \\  [1ex]
male & 76.34 & 41.30 & 40.70 & 43.40 & 35.60 & 74.51 & 67.04 & 60.16 & 50.62 & 60.96 & 52.88 & 61.79 \\  [1ex]
people & 93.91 & 74.80 & 36.10 & 73.10 & 62.80 & 90.77 & 73.32 & 68.36 & 60.37 & 58.19 & 55.56 & 59.18 \\  [1ex]
sea & 98.01 & 52.90 & 36.60 & 47.70 & 25.50 & 93.34 & 73.85 & 88.48 & 53.04 & 77.20 & 73.42 & 81.05 \\  [1ex]
river & 83.35 & 15.80 & 17.90 & 31.70 & 13.00 & 82.39 & 73.58 & 77.14 & 46.68 & 69.73 & 68.55 & 71.10 \\  [1ex]
baby & 98.55 & 20.00 & 8.40 & 28.50 & 6.90 & 95.24 & 73.24 & 77.58 & 47.71 & 72.29 & 73.86 & 80.06 \\  [1ex] \hline
Average & 90.58 & 47.88 & 35.72 & 49.76 & 31.77 & 87.59 & 72.64 & 75.79 & 51.33 & 69.04 & 64.80 & 70.84 \\  [1ex] \hline
\end{tabular}
\label{table:MIRPrecision} % is used to refer this table in the text
\end{sidewaystable}

\newpage
%-------------------- Table example ------------------------------
\begin{sidewaystable}
\caption{ MIR: Accuracy Comparison} % title of Table % used for centering table
\vspace*{0.2 cm}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} \\ \hline  % YOU MAY NEED TO CHANGe This
Labels  & Ensemble Method  &\multicolumn{2}{c}{Social Features}  &\multicolumn{5}{c}{Visual Features}\\ [0.5ex]
 &  & LSI & RP & HOG-LBP & SIFT & GIST & COLOR & GLCM \\  [1ex] \hline

animal & 94.09 & 83.46 & 90.01 & 70.44 & 73.46 & 51.62 & 66.21 & 57.18 & 64.11 \\  [1ex]
coral & 95.45 & 45.73 & 94.82 & 83.10 & 79.45 & 53.11 & 65.22 & 69.23 & 67.64 \\  [1ex]
dancing & 95.98 & 53.66 & 92.82 & 74.07 & 75.59 & 59.13 & 67.39 & 63.30 & 60.74 \\  [1ex]
harbor & 93.63 & 36.47 & 92.24 & 77.58 & 81.40 & 53.22 & 79.32 & 72.40 & 73.07 \\  [1ex]
military & 97.86 & 55.72 & 93.07 & 79.28 & 70.67 & 56.42 & 61.35 & 65.02 & 71.94 \\  [1ex]
mountain & 90.05 & 34.23 & 89.26 & 72.42 & 84.30 & 54.58 & 78.03 & 78.62 & 72.60 \\  [1ex]
snow & 92.32 & 42.43 & 87.54 & 66.07 & 69.11 & 56.66 & 60.82 & 64.92 & 67.05 \\  [1ex]
statue & 88.56 & 54.63 & 84.56 & 73.67 & 57.66 & 54.79 & 59.57 & 52.99 & 56.49 \\  [1ex]
tattoo & 88.92 & 56.24 & 86.81 & 71.95 & 83.43 & 58.64 & 70.74 & 71.13 & 75.49 \\  [1ex]
temple & 93.68 & 43.78 & 89.72 & 69.59 & 64.42 & 54.15 & 57.92 & 58.67 & 58.14 \\  [1ex]
waterfall & 98.58 & 57.38 & 95.88 & 82.59 & 86.45 & 54.65 & 76.68 & 76.56 & 83.82 \\  [1ex]
wedding & 94.22 & 72.34 & 89.98 & 76.31 & 66.43 & 54.44 & 56.91 & 61.50 & 60.27 \\  [1ex] \hline
AVERAGE & 93.61 & 53.01 & 90.56 & 74.76 & 74.36 & 55.12 & 66.68 & 65.96 & 67.61 \\  [1ex] \hline

\end{tabular}
\label{table:MIRAccuracy} % is used to refer this table in the text
\end{sidewaystable}
%--------------------- end of the example ------------------------



\subsection{ImageCLEF}

ImageCLEF has 99 labels. For some labels we have less than 20 instances. So learning visual bag words for such a low number of instance is not favorable. We, therefore, discard such labels. After doing analysis on the available instances of labels and their meta data, we decided to do classification for the following Labels. \\
\textit{'Adult','Aesthetic\_ Impression','Animals','Autumn','Citylife','cute','Day','Flowers','Food',
'Graffiti' ,'Landscape\_ Nature','Painting','Portrait','Single\_ Person','Sky','Street',
'Summer','Sunset\_ Sunrise','Vehicle','Winter'} \\

The ImageCLEF competition \cite{CLEF} has the best comparative published results for judging our hypothesis as it already has results based on Flicker User tags and multi-modal approaches that consider visual information and/or Flicker user tags and/or Exif Information.

Table \ref{table:ImageCLEFPrecision} shows the mean average precision obtained for ImageCLEF Data. 
Table  \ref{table:ImageCLEFAccuracy} shows the average accuracy obtained for ImageCLEF Data. 
 
\subsubsection*{Observations}

\begin{itemize}

\item While comparing MAP for the labels we find that when we use social features with LSI, even then we can  achieve an average improvement of 4.09\% compared to best results (visual, multimodal, textual) used in CLEF competitions.

\item Ensembling of all the features outperforms the published results in precision with average of 6.93\%.

\item When we use the social features with LSI method, even then we can obtain comparable accuracy with the CLEF best results.

\item Ensembling of all the features outperforms the published results in accuracy with average of 1.26\%.

\item For the three labels \textit{Landscape\_ Nature, Sky} and \textit{Sunset\_ sunrise} our method provides lesser accuracy and precision because these labels were more connected to visual data and social data on them were less in amount.

\item For the labels like Autumn, Landscape, Autumn, CityLife, Paintings, Sky, Street and Sunset\_ Sunrise we see that visual feature based computation is works quite well because these images are more visual concept centric and social data has lesser role to play there.

\item HOG-LBP feature works best among all the features for all the labels except painting and single\_ person. In these two cases GLCM works better because GLCM reads the texture of the image and texture of a 'Painting' or 'Single\_ Person' based image plays pivotal role because it is quite different than the normal natural photo clicks.

\end{itemize}

\newpage

\begin{sidewaystable}
\caption{ ImageCLEF: Precision Comparison} % title of Table % used for centering table
\vspace*{0.2 cm}
\begin{tabular}{|c| p{2cm}| p{2cm}| p{2cm}| p{2cm}| p{2cm}|c|c|c|c|} % 10 Columns \\ \hline  % YOU MAY NEED TO CHANGe This
Labels  & Ensemble Method  & CLEF Best Results & \multicolumn{2}{c}{Social Features}  & \multicolumn{5}{c}{Visual Features}\\ [0.5ex] 
 &  & & Latent Semantic Indexing & Random Projection & HOG-LBP & SIFT & GIST & COLOR & GLCM \\  [1ex] \hline
  
Adult & 91.76 & 77.21 & 89.56 & 75.24 & 60.94 & 56.88 & 54.70 & 56.45 & 60.24\\  [1ex]
Aesthetic\_ Impression & 67.74 & 60.66 & 63.91 & 59.30 & 59.58 & 53.33 & 54.94 & 55.60 & 50.82 \\  [1ex]
Animals & 93.76 & 84.34 & 92.83 & 74.05 & 65.67 & 54.52 & 58.87 & 58.65 & 60.09\\  [1ex]
Autumn & 88.12 & 83.51 & 85.89 & 65.67 & 76.01 & 69.43 & 67.71 & 58.74 & 65.18\\  [1ex]
Citylife & 82.02 & 78.37 & 78.47 & 63.79 & 69.87 & 44.81 & 62.13 & 52.60 & 62.63\\  [1ex]
cute & 64.49 & 59.71 & 63.11 & 61.01 & 55.11 & 51.49 & 52.67 & 52.41 & 51.13\\  [1ex]
Day & 87.43 & 80.75 & 84.15 & 68.67 & 67.39 & 54.79 & 59.98 & 63.30 & 61.41\\  [1ex]
Flowers & 94.13 & 82.72 & 93.54 & 69.79 & 73.75 & 52.97 & 65.75 & 65.94 & 57.58\\  [1ex]
Food & 92.30 & 85.20 & 87.51 & 71.04 & 73.30 & 68.41 & 72.84 & 67.89 & 59.68\\  [1ex]
Graffiti & 84.09 & 66.21 & 81.17 & 67.22 & 66.02 & 61.50 & 61.32 & 56.33 & 56.63\\  [1ex]
Landscape\_ Nature & 88.21 & 88.68 & 84.45 & 75.04 & 77.95 & 52.49 & 66.03 & 62.66 & 68.30\\  [1ex]
Painting & 73.04 & 72.43 & 70.51 & 59.10 & 60.26 & 51.96 & 55.70 & 53.26 & 62.49\\  [1ex]
Portrait & 90.27 & 81.34 & 85.82 & 75.59 & 64.98 & 57.47 & 60.80 & 61.90 & 63.16\\  [1ex]
Single\_ Person & 93.99 & 76.41 & 91.25 & 76.48 & 58.97 & 56.14 & 56.37 & 54.80 & 60.28\\  [1ex]
Sky & 88.05 & 89.26 & 87.30 & 76.27 & 80.42 & 54.78 & 73.07 & 69.15 & 67.28\\  [1ex]
Street & 83.41 & 76.76 & 79.20 & 65.91 & 67.86 & 60.37 & 62.71 & 53.38 & 63.46\\  [1ex]
Summer & 75.87 & 74.08 & 71.80 & 65.56 & 62.96 & 52.30 & 56.92 & 59.51 & 59.18\\  [1ex]
Sunset\_ Sunrise & 91.74 & 91.83 & 87.09 & 78.51 & 85.49 & 57.40 & 71.96 & 71.15 & 82.35\\  [1ex]
Vehicle & 88.97 & 78.64 & 87.98 & 69.92 & 74.42 & 51.68 & 62.11 & 58.25 & 69.10\\  [1ex]
Winter & 88.10 & 80.71 & 85.02 & 75.90 & 69.89 & 60.94 & 58.30 & 58.47 & 64.10 \\  [1ex] \hline
Average & 85.37 & 78.44 & 82.53 & 69.70 & 68.54 & 56.18 & 61.74 & 59.52 & 62.25\\  [1ex] \hline
\end{tabular}
\label{table:ImageCLEFPrecision} % is used to refer this table in the text
\end{sidewaystable}

\newpage
%-------------------- Table example ------------------------------
\begin{sidewaystable}
\caption{ ImageCLEF: Accuracy Comparison} % title of Table % used for centering table
\vspace*{0.2 cm}
\begin{tabular}{|c| p{2cm}| p{2cm}| p{2cm}| p{2cm}| p{2cm}|c|c|c|c|} % 10 Columns \\ \hline  % YOU MAY NEED TO CHANGe This
Labels  & Ensemble Method & ImageCLEF Competetion Results &\multicolumn{2}{c}{Social Features}  &\multicolumn{5}{c}{Visual Features}\\ [0.5ex] \hline % YOU MAY NEED TO CHANGe This
 &  & & Latent Semantic Indexing & Random Projection & HOG-LBP & SIFT & GIST & COLOR & GLCM \\  [1ex] \hline
Adult & 90.03 & 80.73 & 88.50 & 75.17 & 60.33 & 55.50 & 53.83 & 56.33 & 60.67 \\  [1ex]
Aesthetic\_ Impression & 68.01 & 60.89 & 64.83 & 58.83 & 59.33 & 45.33 & 54.50 & 54.83 & 48.83 \\  [1ex]
Animals & 92.13 & 88.43 & 90.17 & 73.17 & 65.67 & 53.17 & 58.67 & 58.00 & 59.33 \\  [1ex]
Autumn & 86.16 & 88.31 & 83.57 & 62.14 & 74.29 & 66.43 & 63.57 & 57.86 & 63.57 \\  [1ex]
Citylife & 81.02 & 82.67 & 78.00 & 63.50 & 69.83 & 47.67 & 62.83 & 52.33 & 62.83 \\  [1ex]
cute & 65.72 & 59.24 & 63.00 & 60.17 & 55.17 & 49.50 & 52.17 & 52.00 & 47.00 \\  [1ex]
Day & 82.82 & 85.17 & 82.17 & 67.17 & 66.83 & 49.33 & 58.67 & 62.67 & 62.00 \\  [1ex]
Flowers & 91.74 & 87.03 & 89.75 & 69.75 & 72.25 & 52.25 & 65.75 & 65.25 & 56.75 \\  [1ex]
Food & 87.36 & 88.70 & 86.00 & 71.67 & 73.33 & 51.67 & 72.33 & 67.00 & 60.33 \\  [1ex]
Graffiti & 75.90 & 69.85 & 75.00 & 63.57 & 56.43 & 50.71 & 57.14 & 53.57 & 56.43 \\  [1ex]
Landscape\_ Nature & 84.69 & 91.54 & 83.67 & 73.67 & 77.00 & 51.50 & 66.00 & 60.83 & 67.50 \\  [1ex]
Painting & 71.96 & 76.28 & 69.17 & 57.78 & 60.00 & 51.67 & 50.28 & 53.06 & 61.39 \\  [1ex]
Portrait & 90.50 & 85.59 & 86.67 & 75.17 & 66.00 & 52.00 & 60.33 & 62.00 & 63.33 \\  [1ex]
Single\_ Person & 91.89 & 80.04 & 91.33 & 75.00 & 58.33 & 47.17 & 55.67 & 54.33 & 59.67 \\  [1ex]
Sky & 87.36 & 91.75 & 86.33 & 76.50 & 79.50 & 52.67 & 71.83 & 67.33 & 65.50 \\  [1ex]
Street & 79.52 & 81.20 & 78.50 & 65.67 & 67.83 & 50.83 & 63.33 & 53.00 & 64.17 \\  [1ex]
Summer & 71.97 & 78.50 & 71.00 & 65.00 & 62.67 & 52.00 & 56.83 & 58.33 & 59.67 \\  [1ex]
Sunset\_ Sunrise & 88.24 & 93.24 & 86.84 & 77.89 & 85.00 & 55.00 & 72.37 & 70.53 & 81.05 \\  [1ex]
Vehicle & 88.50 & 82.38 & 87.50 & 70.00 & 73.50 & 48.33 & 61.83 & 57.83 & 69.33 \\  [1ex]
Winter & 86.48 & 85.36 & 84.58 & 76.25 & 70.00 & 57.50 & 58.75 & 56.25 & 64.17 \\  [1ex] \hline
Average & 83.10 & 81.84 & 81.33 & 68.90 & 67.66 & 52.01 & 60.83 & 58.67 & 61.68 \\  [1ex] \hline

\end{tabular}
\label{table:ImageCLEFAccuracy} % is used to refer this table in the text
\end{sidewaystable}
%--------------------- end of the example ------------------------

\subsection{PASCAL}

PASCAL data-set is actually a competition which is based on the challenge of recognizing visual object classes in realistic scenes. Therefore, the data-set is actually very much visual content specific and has image instances having information specifically related to these objects. We can actually subclass the 19 labels, we used in our computations, in following 3 sub classes.

\begin{itemize}
\item Animal: bird, cat, cow, dog, horse, sheep
\item Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train
\item Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor

\end{itemize}

Table \ref{table:PASCALPrecision} shows the MAP comparison of various labels and also comparison with the VOC Competition results.
Table \ref{table:PASCALAccuracy} shows the accuracy of classifying various labels in binary prediction environment.


\subsubsection*{Observations}


\begin{itemize}
\item Observations:

\item The classification based on only visual features gives an average precision of 67.50\% with maximum for 'aeroplane' of 78.79\% and minimum for 'potted plant' of 59.72\%.

\item The classification based on only LSI computed social features outperforms the competition's results with an average of 16.74\% better MAP and only visual features result with 6.17\%. The average precision obtained is 73.67\%.

\item Fusion of social and visual features provides a better precision of 76.86\% which is 3.19\% more than only social features and 9.36\% more than usual visual classification.

\item Our ensemble method and social feature based method gives better precision for 17 labels out of 19 labels considered compared to published results.

\item The accuracy obtained with using only visual features is average of 66.68\%. With minimum 60.67\% for 'cat' and maximum of 73.50\% for 'bottle'.

\item While comparing the accuracy of our classification, we find that accuracy obtained using LSI based social features is 70.22\%, which is greater than 3.54\% for the visual only features.

\item The fusion of social and visual features provides accuracy of 72.49 \% which is 2.7
1\% more than only social features based classification and 5.81\% more than only visual features based classification.

\item The accuracy for visual only features of the most labels are comparable with social features and not have difference more than 3\%. This difference is much higher for all other three data-set s. This low difference shows that the social data available for the PASCAL Data-set is not as enriched as other data-sets. The images are not contextually interesting as compared to other data-sets, so do not encountered with much human interaction leading to lesser social data.

\end{itemize}

\newpage

\begin{sidewaystable}
\caption{ PASCAL: Precision Comparison} % title of Table % used for centering table
\vspace*{0.2 cm}
\begin{tabular}{|c|c| p{2cm}| p{2cm}|c|c|c|c|c|c|} % 10 Columns \\ \hline  % YOU MAY NEED TO CHANGe This
Labels  & Ensemble Method  & PASCAL VOC Results & \multicolumn{2}{c}{Social Features}  & \multicolumn{5}{c}{Visual Features}\\ [0.5ex] 
 &  & & Latent Semantic Indexing & Random Projection & HOG-LBP & SIFT & GIST & COLOR & GLCM \\  [1ex] \hline
  
aeroplane & 85.83 & 77.50 & 81.76 & 70.60 & 75.42 & 56.53 & 72.82 & 65.20 & 78.79 \\  [1ex]
bicycle & 68.29 & 63.60 & 67.65 & 60.58 & 67.51 & 55.38 & 60.60 & 53.57 & 62.41 \\  [1ex]
bird & 82.38 & 56.10 & 79.22 & 69.80 & 62.49 & 55.64 & 65.89 & 50.86 & 65.30 \\  [1ex]
boat & 68.54 & 71.90 & 63.67 & 54.88 & 67.14 & 57.14 & 63.19 & 57.79 & 62.99 \\  [1ex]
bottle & 71.09 & 33.10 & 66.30 & 61.95 & 57.54 & 48.67 & 55.70 & 57.80 & 62.44 \\  [1ex] 
bus & 77.58 & 60.60 & 76.79 & 63.74 & 74.26 & 56.43 & 66.24 & 57.97 & 66.13 \\  [1ex]
car & 73.50 & 78.00 & 68.71 & 56.02 & 61.25 & 54.20 & 58.41 & 54.69 & 60.63 \\  [1ex]
cat & 81.73 & 58.80 & 77.73 & 68.49 & 69.36 & 56.27 & 64.01 & 59.97 & 68.32 \\  [1ex]
chair & 73.87 & 53.50 & 71.76 & 61.50 & 61.19 & 54.38 & 51.94 & 57.86 & 61.14 \\  [1ex]
cow & 75.59 & 42.60 & 71.63 & 63.67 & 68.83 & 56.62 & 61.20 & 60.39 & 64.33 \\  [1ex]
diningtable & 85.35 & 54.90 & 82.07 & 70.89 & 67.67 & 62.70 & 65.11 & 57.66 & 62.59 \\  [1ex]
dog & 83.18 & 45.80 & 78.93 & 64.97 & 65.07 & 49.69 & 61.33 & 55.64 & 64.52 \\  [1ex]
horse & 82.19 & 77.50 & 78.80 & 65.44 & 67.49 & 46.98 & 58.99 & 56.93 & 62.26 \\  [1ex]
motorbike & 80.51 & 64.00 & 76.80 & 59.73 & 66.52 & 58.15 & 68.11 & 55.44 & 62.67 \\  [1ex]
pottedplant & 67.87 & 36.30 & 64.34 & 57.02 & 59.60 & 53.57 & 55.97 & 59.72 & 57.10 \\  [1ex]
sheep & 80.13 & 44.70 & 78.75 & 65.73 & 73.34 & 49.68 & 67.70 & 62.14 & 68.27 \\  [1ex]
sofa & 70.64 & 50.90 & 70.16 & 63.35 & 64.89 & 53.21 & 56.27 & 54.22 & 62.71 \\  [1ex]
train & 86.96 & 79.20 & 83.48 & 69.57 & 71.61 & 56.76 & 65.79 & 57.61 & 63.24 \\  [1ex]
tvmonitor & 74.00 & 53.20 & 69.25 & 61.77 & 66.90 & 57.38 & 67.69 & 59.13 & 67.84 \\  [1ex] \hline
Average  & 76.86 & 56.93 & 73.67 & 63.28 & 66.26 & 54.60 & 61.90 & 57.19 & 63.61 \\  [1ex]

\hline %inserts single line
\end{tabular}
\label{table:PASCALPrecision} % is used to refer this table in the text
\end{sidewaystable}


\newpage
%-------------------- Table example ------------------------------
\begin{sidewaystable}
\caption{ PASCAL: Accuracy Comparison} % title of Table % used for centering table
\vspace*{0.2 cm}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|} % 9 Columns \\ \hline  % YOU MAY NEED TO CHANGe This
Labels  & Ensemble Method  &\multicolumn{2}{c}{Social Features}  &\multicolumn{5}{c}{Visual Features}\\ [0.5ex] \hline % YOU MAY NEED TO CHANGe This
 &  & Latent Semantic Indexing & Random Projection & HOG-LBP & SIFT & GIST & COLOR & GLCM \\  [1ex] \hline
aeroplane & 78.46 & 74.5 & 69 & 73.67 & 47.33 & 70.67 & 64.17 & 74.83\\  [1ex]
bicycle & 69.82 & 65.17 & 59.17 & 66.17 & 54.67 & 60.17 & 53.17 & 62.5\\  [1ex]
bird & 75.22 & 74.83 & 68.33 & 62 & 55.67 & 64.83 & 51 & 63.33\\  [1ex]
boat & 67.85 & 62.5 & 54.83 & 65.67 & 52 & 62.67 & 57 & 61\\  [1ex]
bottle & 67.69 & 63.83 & 60.33 & 56.33 & 48.67 & 55.33 & 52.17 & 61.83\\  [1ex]
bus & 77.38 & 71.5 & 62.83 & 73.5 & 55.5 & 66 & 57.5 & 65.83\\  [1ex]
car & 69.77 & 67.83 & 56 & 61.5 & 49.5 & 57.67 & 54.17 & 60.5\\  [1ex]
cat & 74.07 & 73.5 & 68 & 68.33 & 55.83 & 63.83 & 59.5 & 67\\  [1ex]
chair & 72 & 68.33 & 60 & 60.67 & 54 & 51.33 & 56.83 & 60.5\\  [1ex]
cow & 73.17 & 69.33 & 62.83 & 68.83 & 51.5 & 61.33 & 60 & 63.67\\  [1ex]
diningtable & 75.81 & 75.67 & 68 & 67 & 47.83 & 63.33 & 57.33 & 63\\  [1ex]
dog & 77.9 & 74.17 & 63.17 & 64.5 & 47 & 60.83 & 54.67 & 64.83\\  [1ex]
horse & 76.7 & 73.67 & 64 & 67.83 & 48 & 59.17 & 56.33 & 61.17\\  [1ex]
motorbike & 73.74 & 72.17 & 58.67 & 66.33 & 57.17 & 66.67 & 55.67 & 62.67\\  [1ex]
person & 59.85 & 56.33 & 52.33 & 59.17 & 52 & 52.33 & 51.33 & 55.33\\  [1ex]
pottedplant & 61.79 & 61.67 & 56.17 & 59.33 & 53 & 55.17 & 52.5 & 56.5\\  [1ex]
sheep & 74.85 & 74.67 & 64.5 & 72.83 & 50.17 & 67.67 & 61.5 & 67.17\\  [1ex]
sofa & 68.96 & 65.67 & 62.17 & 63.83 & 52.17 & 56 & 52.5 & 63.5\\  [1ex]
train & 80.1 & 78.83 & 68.5 & 70.5 & 49.5 & 65.17 & 57.33 & 63.33\\  [1ex]
tvmonitor & 67.97 & 66.33 & 59.83 & 65.83 & 51.33 & 67 & 57.83 & 67.83\\  [1ex]  \hline 
Average & 72.16 & 69.53 & 61.93 & 65.69 & 51.64 & 61.36 & 56.13 & 63.32 \\  [1ex] \hline  
\end{tabular}
\label{table:PASCALAccuracy} % is used to refer this table in the text
\end{sidewaystable}
%--------------------- end of the example ------------------------




\subsection{NUS}
In \cite{NUS}, authors have used six types of low-level features extracted. These six type of image features include 64-D color histogram, 144-D color correlogram, 73-D edge direction histogram, 128-D wavelet texture, 225-D block-wise color moments and 500-D bag of words based on SIFT descriptions. Further they have used traditional k-NN algorithm  on these features to provide the baseline results for web image annotation. We use the results of this paper as a comparison base. 

Table \ref{table:NUSPrecision} shows the MAP comparison of various labels and also comparison with the VOC Competition results.
Table \ref{table:NUSAccuracy} shows the accuracy of classifying various labels in binary prediction environment.


\subsubsection*{Observations}

\begin{itemize}
\item The classifications based on visual only features give the average precision of 74.63\% which is better with the large margin of 10.13\% from the published results in \cite{NUS}.

\item The classifications based on social only features (with LSI pre-computation) give the average precision of 90.56\% outperforming the results obtained from the visual only features with the margin of 15.93\%.

\item Fusion of all the features and ensemble classification leads to the average precision of 93.61\%, which  is better than precision of social only features with 3.05\% margin. This result is way better than baseline results mentioned in \cite{NUS} and shows a margin of 29.11\% in the precision.

\item The classifications based on visual only features give the average accuracy of 74.84\%.

\item The classifications based on social only features (with LSI pre-computation) give the average accuracy of 88.61\% outperforming the results obtained from the visual only features with the margin of 13.77\%.

\item Fusion of all the features and ensemble classification leads to the average accuracy of 91.23\%, which is better than accuracy of social only features with 2.62\% margin. 

\item The results of social features and image features are quite close for “mountain”, “tattoo”. This is the result of a great degree of visual information present with these images, compared to social data.
\end{itemize}

\newpage
\begin{sidewaystable}
\caption{ NUS: Precision Comparison} % title of Table % used for centering table
\vspace*{0.2 cm}
\begin{tabular}{|c|c| p{2cm}| p{2cm}|c|c|c|c|c|c|} % 10 Columns \\ \hline  % YOU MAY NEED TO CHANGe This
Labels  & Ensemble Method  &  NUS Baseline Results & \multicolumn{2}{c}{Social Features}  & \multicolumn{5}{c}{Visual Features}\\ [0.5ex] 
 &  & & Latent Semantic Indexing & Random Projection & HOG-LBP & SIFT & GIST & COLOR & GLCM \\  [1ex] \hline
animal & 94.09 & 83.46 & 90.01 & 70.44 & 73.46 & 51.62 & 66.21 & 57.18 & 64.11 \\  [1ex]
coral & 95.45 & 45.73 & 94.82 & 83.10 & 79.45 & 53.11 & 65.22 & 69.23 & 67.64 \\  [1ex]
dancing & 95.98 & 53.66 & 92.82 & 74.07 & 75.59 & 59.13 & 67.39 & 63.30 & 60.74 \\  [1ex]
harbor & 93.63 & 36.47 & 92.24 & 77.58 & 81.40 & 53.22 & 79.32 & 72.40 & 73.07 \\  [1ex]
military & 97.86 & 55.72 & 93.07 & 79.28 & 70.67 & 56.42 & 61.35 & 65.02 & 71.94 \\  [1ex]
mountain & 90.05 & 34.23 & 89.26 & 72.42 & 84.30 & 54.58 & 78.03 & 78.62 & 72.60 \\  [1ex]
snow & 92.32 & 42.43 & 87.54 & 66.07 & 69.11 & 56.66 & 60.82 & 64.92 & 67.05 \\  [1ex]
statue & 88.56 & 54.63 & 84.56 & 73.67 & 57.66 & 54.79 & 59.57 & 52.99 & 56.49 \\  [1ex]
tattoo & 88.92 & 56.24 & 86.81 & 71.95 & 83.43 & 58.64 & 70.74 & 71.13 & 75.49 \\  [1ex]
temple & 93.68 & 43.78 & 89.72 & 69.59 & 64.42 & 54.15 & 57.92 & 58.67 & 58.14 \\  [1ex]
waterfall & 98.58 & 57.38 & 95.88 & 82.59 & 86.45 & 54.65 & 76.68 & 76.56 & 83.82 \\  [1ex]
wedding & 94.22 & 72.34 & 89.98 & 76.31 & 66.43 & 54.44 & 56.91 & 61.50 & 60.27 \\  [1ex] \hline
AVERAGE & 93.61 & 53.01 & 90.56 & 74.76 & 74.36 & 55.12 & 66.68 & 65.96 & 67.61 \\  [1ex] \hline
  
\end{tabular}
\label{table:NUSPrecision} % is used to refer this table in the text
\end{sidewaystable}


\newpage
%-------------------- Table example ------------------------------
\begin{sidewaystable}[ht]
\caption{NUS: Accuracy Comparison} % title of Table % used for centering table
\vspace*{0.2 cm}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|} % centered columns (4 columns) \\ \hline 
Labels  & Ensemble Method  &\multicolumn{2}{c}{Social Features}  &\multicolumn{5}{c}{Visual Features}\\ [0.5ex] 
%\hline\hline %inserts double horizontal lines
%Label Names & Ensemble method & \multicolumn{2}{Social Features} & \multicolumn{5}{Visual Features} \\ \hline % inserts single-line
  &  & Latent Semantic Indexing & Random Projection & HOG-LBP & SIFT & GIST & COLOR & GLCM \\  [1ex]
  \hline
animal & 91.62 & 88.00 & 70.17 & 72.86 & 50.54 & 66.07 & 56.96 & 64.29 \\  [1ex]
coral & 95.99 & 92.33 & 80.83 & 79.63 & 52.59 & 65.56 & 68.33 & 67.41 \\  [1ex]
dancing & 92.39 & 92.00 & 74.33 & 75.58 & 58.85 & 67.88 & 62.88 & 60.96 \\  [1ex]
harbor & 93.02 & 90.83 & 75.83 & 82.96 & 53.15 & 80.37 & 72.59 & 73.15 \\  [1ex]
military & 94.69 & 90.83 & 77.00 & 69.81 & 55.74 & 61.11 & 64.26 & 70.93 \\  [1ex]
mountain & 90.55 & 86.67 & 72.67 & 85.00 & 53.33 & 78.33 & 76.85 & 71.85 \\  [1ex]
snow & 88.44 & 86.50 & 66.00 & 70.19 & 47.59 & 60.93 & 64.63 & 65.00 \\  [1ex]
statue & 85.23 & 84.67 & 72.50 & 58.15 & 54.07 & 59.26 & 52.59 & 55.93 \\  [1ex]
tattoo & 88.83 & 85.17 & 71.33 & 84.07 & 57.04 & 70.93 & 71.67 & 77.41 \\  [1ex]
temple & 91.50 & 87.67 & 68.83 & 65.00 & 53.52 & 58.15 & 58.52 & 57.96 \\  [1ex]
waterfall & 93.98 & 93.83 & 81.17 & 86.67 & 53.70 & 75.93 & 76.48 & 84.07 \\  [1ex]
wedding & 88.57 & 84.83 & 74.67 & 65.93 & 53.33 & 56.67 & 60.74 & 60.93 \\  [1ex]
\hline
Average & 91.23 & 88.61 & 73.78 & 74.65 & 53.62 & 66.76 & 65.54 & 67.49 \\  [1ex]
\hline %inserts single line
\end{tabular}
\label{table:NUSAccuracy} % is used to refer this table in the text
\end{sidewaystable}
%--------------------- end of the example ------------------------

